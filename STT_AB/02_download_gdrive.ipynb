{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../util')\n",
    "\n",
    "from util import read_spreadsheet\n",
    "from util import collect_segments\n",
    "from util import download_audio_gdrive\n",
    "from util import split_audio_files\n",
    "from util import get_time_span\n",
    "from util import get_max_db_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U yt-dlp[default]                     # Install yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_spreadsheet(sheet_id = \"1yKSzConuVWo8BuMDs2mabF5iiBKUz2wF--LIabFN6QE\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    file_name = row['ID']\n",
    "    gd_url = row['Audio link']\n",
    "    Sr_no = row['z']\n",
    "    \n",
    "    # if not isinstance(gd_url, str) or not isinstance(id, str):\n",
    "    #     continue\n",
    "    if Sr_no >= 645 and Sr_no <= 691:\n",
    "        gd_url = gd_url.split('?')[0]\n",
    "        print(gd_url, file_name)\n",
    "        # file_name = file_name + \".mp3\"\n",
    "        # download_audio_gdrive(gd_url, file_name)\n",
    "        yt_downloaded = f\"\"\"yt-dlp --extract-audio --audio-quality 0 --audio-format wav --postprocessor-args \"-ar 16000 -ac 1\" {gd_url} -o './full_audio/{file_name}.%(ext)s'\"\"\"\n",
    "        ! {yt_downloaded}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_audio_files('STT_AB', 'wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_segments('STT_AB', 'after_split', 'segments_ab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp segments_ab  s3://monlam.ai.stt/wav16k/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_directory = f\"segments_ab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "target_path = Path(target_directory)\n",
    "\n",
    "rows = []\n",
    "generator = pipeline(model=\"openpecha/wav2vec2_run8\")\n",
    "\n",
    "for file in tqdm(target_path.glob('*.wav'), total=len(list(target_path.glob('*.wav')))):\n",
    "    inf = generator(str(file))[\"text\"]\n",
    "    rows.append([file.stem, f\"https://d38pmlk0v88drf.cloudfront.net/wav16k/{file.name}\", inf, get_time_span(str(file.name))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(rows, columns =['file_name', 'url', 'inference_transcript', 'audio_duration'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['inference_transcript','url']].iloc[0:10].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"stt_ab_from_yt.tsv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from fast_antx.core import transfer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_tsv_text(tsvFile, ColumnNumber):\n",
    "    \"\"\"extracts text from dataframe using column number\n",
    "    Args:\n",
    "        tsvFile (Dataframe): dataframe of predicted tsv file\n",
    "        ColumnNumber (integer/string):column name of the text to be extracted\n",
    "\n",
    "    Returns:\n",
    "        string: extracted text from tsv file\n",
    "    \"\"\"\n",
    "    # read the tsv file\n",
    "    predictedText = tsvFile[ColumnNumber].tolist()\n",
    "    # to avoid unwanted splits in a word we replace space with _\n",
    "    for count, text in enumerate(predictedText):\n",
    "        predictedText[count] = predictedText[count].replace(\" \", \"_\")\n",
    "    predictedText = \"\\n\".join(\" \".join(predictedText).split())\n",
    "    print(\"extracted text from tsv file..\")\n",
    "    return predictedText\n",
    "\n",
    "\n",
    "def get_original_text(OriginalText):\n",
    "    \"\"\"reads the original text and removes unwanted characters\n",
    "\n",
    "    Args:\n",
    "        OriginalText (string): location of the original text file\n",
    "\n",
    "    Returns:\n",
    "        string: original text without unwanted characters\n",
    "    \"\"\"\n",
    "    target = Path(f\"{OriginalText}\").read_text(encoding=\"utf-8\")\n",
    "    # remove unwanted characters\n",
    "    target = target.replace(\"“\", \"\").replace(\"”\", \"\")\n",
    "    print(\"extracted text from original file..\")\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "def transfer_text(OriginalText, PredictedTSV, file_name, ColumnNumber='inference_transcript'):\n",
    "    \"\"\"transfers the annotation from predicted text to original text and returns a dataFrame\n",
    "\n",
    "    Args:\n",
    "        OriginalText (string): location of the original string\n",
    "        PredictedTSV (string): location of the predicted tsv file\n",
    "        ColumnNumber (int/string): name of the column in which transcribed text is there in .tsv file\n",
    "\n",
    "    Returns:\n",
    "        dataFrame: dataFrame that contains transferred annotation on original text\n",
    "    \"\"\"\n",
    "    tsvFile = pd.read_csv(f\"{PredictedTSV}\", sep=\"\\t\")\n",
    "    tsvFile = tsvFile[tsvFile['file_name'].str[0:11] == file_name]\n",
    "\n",
    "    tsvFile.sort_values(by=['file_name'], inplace=True)\n",
    "\n",
    "    source = extract_tsv_text(tsvFile, ColumnNumber)\n",
    "    target = get_original_text(OriginalText)\n",
    "    annotation = [[\"segment\", \"(\\n)\"]]\n",
    "    transferredText = transfer(source, annotation, target).split(\"\\n\")\n",
    "    if len(transferredText) > len(tsvFile):\n",
    "        transferredText = transferredText[:len(tsvFile)]\n",
    "        tsvFile[ColumnNumber] = transferredText\n",
    "        status= f'Truncated {abs(len(transferredText)-len(tsvFile))}'\n",
    "    elif len(transferredText) < len(tsvFile):\n",
    "        transferredText = transferredText + [np.nan]*(len(tsvFile) - len(transferredText))\n",
    "        tsvFile[ColumnNumber] = transferredText\n",
    "        status=f'Padded {abs(len(transferredText)-len(tsvFile))}'\n",
    "    else:\n",
    "        tsvFile[ColumnNumber] = transferredText\n",
    "        status='Normal'\n",
    "\n",
    "    # returns a dataFrame\n",
    "    return tsvFile, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for file_name in [f\"STT_AB00{x}\" for x in range(645,692)]:\n",
    "    transfer_text_df, status = transfer_text(f'etexts/{file_name}.txt',f'stt_ab_from_yt.tsv', file_name)\n",
    "    temp.append(transfer_text_df)\n",
    "    print(status)\n",
    "df = pd.concat(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ab_ga_id = 1\n",
    "group_ab_gb_id = 2\n",
    "group_ab_gc_id = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['group_id'] = group_ab_ga_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['state'] = 'transcribing'\n",
    "df.fillna('', inplace=True)\n",
    "def filter_length(st):\n",
    "    return len(st) < 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['inference_transcript'].apply(lambda x: len(x) < 500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('file_name')\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_db_id = get_max_db_id()\n",
    "\n",
    "id_arr = list(range(last_db_id + 1, df.shape[0] + last_db_id + 1))\n",
    "\n",
    "df['id'] = id_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('stt_ab_upload_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
