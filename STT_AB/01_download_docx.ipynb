{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../util')\n",
    "\n",
    "from util import read_spreadsheet\n",
    "from util import get_max_db_id\n",
    "from util import collect_segments\n",
    "from util import download_audio_gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Using cached python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
      "Collecting lxml>=3.1.0\n",
      "  Downloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /media/monlamai/SSD/GitHub/split/.env/lib/python3.10/site-packages (from python-docx) (4.8.0)\n",
      "Installing collected packages: lxml, python-docx\n",
      "Successfully installed lxml-5.1.0 python-docx-1.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "from docx import Document\n",
    "\n",
    "def docx_to_txt(docx_path):\n",
    "        doc = Document(docx_path)\n",
    "        fullText = []\n",
    "        for para in doc.paragraphs:\n",
    "            fullText.append(para.text)\n",
    "        return '\\n'.join(fullText)\n",
    "\n",
    "def download_etext(gd_url,file_name):\n",
    "    if not os.path.exists('etexts'):\n",
    "        os.makedirs('etexts')\n",
    "\n",
    "    if not os.path.exists('docx'):\n",
    "        os.makedirs('docx')\n",
    "    \n",
    "    if os.path.exists(f'etexts/{file_name}.txt'):\n",
    "        return\n",
    "    docx_url, _ = os.path.split(gd_url)\n",
    "    docx_url = os.path.join(docx_url, 'export?format=docx')\n",
    "    docx_path = gdown.download(docx_url, output=f'docx/{file_name}.docx', quiet=False, fuzzy=True)\n",
    "    # Convert the .docx file to text\n",
    "    text = docx_to_txt(docx_path)\n",
    "    # Create a .txt path with the same name as the .docx file\n",
    "    txt_path = os.path.join('etexts/', file_name + '.txt')\n",
    "    # Save the text to a .txt file\n",
    "    with open(txt_path, 'w',encoding='utf-8') as f:\n",
    "        f.write(text.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STT_AB00595 https://docs.google.com/document/d/1TqSCgaVpB2K8hi7srpOKS6M4O5uLiFH0N2KXfguZBhA/edit 595.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://docs.google.com/document/d/1TqSCgaVpB2K8hi7srpOKS6M4O5uLiFH0N2KXfguZBhA/export?format=docx\n",
      "To: /media/monlamai/SSD/GitHub/split/STT_AB/docx/STT_AB00595.docx\n",
      "9.22kB [00:00, 246kB/s]\n"
     ]
    }
   ],
   "source": [
    "df = read_spreadsheet(sheet_id=\"1yKSzConuVWo8BuMDs2mabF5iiBKUz2wF--LIabFN6QE\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    file_name = row['ID']\n",
    "    gd_url = row['Etext link']\n",
    "    Sr_no = row['z']\n",
    "    # if not isinstance(gd_url, str) or not isinstance(id, str):\n",
    "    #     continue\n",
    "    if Sr_no >= 595 and Sr_no <= 644:\n",
    "        print(file_name, gd_url, Sr_no)\n",
    "        download_etext(gd_url, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from fast_antx.core import transfer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_tsv_text(tsvFile, ColumnNumber):\n",
    "    \"\"\"extracts text from dataframe using column number\n",
    "    Args:\n",
    "        tsvFile (Dataframe): dataframe of predicted tsv file\n",
    "        ColumnNumber (integer/string):column name of the text to be extracted\n",
    "\n",
    "    Returns:\n",
    "        string: extracted text from tsv file\n",
    "    \"\"\"\n",
    "    # read the tsv file\n",
    "    predictedText = tsvFile[ColumnNumber].tolist()\n",
    "    # to avoid unwanted splits in a word we replace space with _\n",
    "    for count, text in enumerate(predictedText):\n",
    "        predictedText[count] = predictedText[count].replace(\" \", \"_\")\n",
    "    predictedText = \"\\n\".join(\" \".join(predictedText).split())\n",
    "    print(\"extracted text from tsv file..\")\n",
    "    return predictedText\n",
    "\n",
    "\n",
    "def get_original_text(OriginalText):\n",
    "    \"\"\"reads the original text and removes unwanted characters\n",
    "\n",
    "    Args:\n",
    "        OriginalText (string): location of the original text file\n",
    "\n",
    "    Returns:\n",
    "        string: original text without unwanted characters\n",
    "    \"\"\"\n",
    "    target = Path(f\"{OriginalText}\").read_text(encoding=\"utf-8\")\n",
    "    # remove unwanted characters\n",
    "    target = target.replace(\"“\", \"\").replace(\"”\", \"\")\n",
    "    print(\"extracted text from original file..\")\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "def transfer_text(OriginalText, PredictedTSV, file_name, ColumnNumber='inference_transcript'):\n",
    "    \"\"\"transfers the annotation from predicted text to original text and returns a dataframe\n",
    "\n",
    "    Args:\n",
    "        OriginalText (string): location of the original string\n",
    "        PredictedTSV (string): location of the predicted tsv file\n",
    "        ColumnNumber (int/string): name of the coloumn in which transcripted text is there in .tsv file\n",
    "\n",
    "    Returns:\n",
    "        dataframe: dataframe that contains transfered annotation on original text\n",
    "    \"\"\"\n",
    "    tsvFile = pd.read_csv(f\"{PredictedTSV}\", sep=\"\\t\")\n",
    "    tsvFile = tsvFile[tsvFile['file_name'].str[0:11] == file_name]\n",
    "\n",
    "    tsvFile.sort_values(by=['file_name'], inplace=True)\n",
    "\n",
    "    source = extract_tsv_text(tsvFile, ColumnNumber)\n",
    "    target = get_original_text(OriginalText)\n",
    "    annotation = [[\"segment\", \"(\\n)\"]]\n",
    "    transferedText = transfer(source, annotation, target).split(\"\\n\")\n",
    "    if len(transferedText) > len(tsvFile):\n",
    "        transferedText = transferedText[:len(tsvFile)]\n",
    "        tsvFile[ColumnNumber] = transferedText\n",
    "        status= f'Truncated {abs(len(transferedText)-len(tsvFile))}'\n",
    "    elif len(transferedText) < len(tsvFile):\n",
    "        transferedText = transferedText + [np.nan]*(len(tsvFile) - len(transferedText))\n",
    "        tsvFile[ColumnNumber] = transferedText\n",
    "        status=f'Padded {abs(len(transferedText)-len(tsvFile))}'\n",
    "    else:\n",
    "        tsvFile[ColumnNumber] = transferedText\n",
    "        status='Normal'\n",
    "\n",
    "    # returns a dataframe\n",
    "    return tsvFile, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n",
      "extracted text from tsv file..\n",
      "extracted text from original file..\n",
      "Normal\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for file_name in [f\"STT_AB00{x}\" for x in range(595,645)]:\n",
    "    transfer_text_df, status = transfer_text(f'etexts/{file_name}.txt',f'stt_ab_from_gdrive.tsv', file_name)\n",
    "    temp.append(transfer_text_df)\n",
    "    print(status)\n",
    "df = pd.concat(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>url</th>\n",
       "      <th>inference_transcript</th>\n",
       "      <th>audio_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7537</th>\n",
       "      <td>STT_AB00595_0001_21936_to_27346</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/stt_pech...</td>\n",
       "      <td>འགྲུལ་པ་དང་ལམ། ནང་མི་ཞིག་སྒེར་གྱི་མོ་ཊ་ཁྲིད་ནས...</td>\n",
       "      <td>5.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>STT_AB00595_0002_28250_to_37056</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/stt_pech...</td>\n",
       "      <td>རྒྱབ་ཏུ་ཨ་མ་བཟང་མོ། ཕྲུ་གུ་འབྲིང་བ་བུ་དོན་གྲུ...</td>\n",
       "      <td>8.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>STT_AB00595_0003_38523_to_41988</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/stt_pech...</td>\n",
       "      <td>ཕྲུ་གུ་ཚོའི་དབར་ལ་ལོ་གསུམ་རེའི་་ཁྱད་པར་ཡོད་པ་...</td>\n",
       "      <td>3.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8450</th>\n",
       "      <td>STT_AB00595_0004_59854_to_64411</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/stt_pech...</td>\n",
       "      <td>་ལོ་ན་བཞི་བཅུ་ཡིན་པའི་ཕ་ཉ</td>\n",
       "      <td>4.557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>STT_AB00595_0005_77875_to_87295</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/stt_pech...</td>\n",
       "      <td>ི་མ་མོ་ཊ་གཏོང་རྒྱུར་དབྱིངས་འཕར་ཏེ་མགྱོགས་པོ་གཏ...</td>\n",
       "      <td>9.420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file_name  \\\n",
       "7537  STT_AB00595_0001_21936_to_27346   \n",
       "767   STT_AB00595_0002_28250_to_37056   \n",
       "3194  STT_AB00595_0003_38523_to_41988   \n",
       "8450  STT_AB00595_0004_59854_to_64411   \n",
       "869   STT_AB00595_0005_77875_to_87295   \n",
       "\n",
       "                                                    url  \\\n",
       "7537  https://d38pmlk0v88drf.cloudfront.net/stt_pech...   \n",
       "767   https://d38pmlk0v88drf.cloudfront.net/stt_pech...   \n",
       "3194  https://d38pmlk0v88drf.cloudfront.net/stt_pech...   \n",
       "8450  https://d38pmlk0v88drf.cloudfront.net/stt_pech...   \n",
       "869   https://d38pmlk0v88drf.cloudfront.net/stt_pech...   \n",
       "\n",
       "                                   inference_transcript  audio_duration  \n",
       "7537  འགྲུལ་པ་དང་ལམ། ནང་མི་ཞིག་སྒེར་གྱི་མོ་ཊ་ཁྲིད་ནས...           5.410  \n",
       "767    རྒྱབ་ཏུ་ཨ་མ་བཟང་མོ། ཕྲུ་གུ་འབྲིང་བ་བུ་དོན་གྲུ...           8.806  \n",
       "3194   ཕྲུ་གུ་ཚོའི་དབར་ལ་ལོ་གསུམ་རེའི་་ཁྱད་པར་ཡོད་པ་...           3.465  \n",
       "8450                          ་ལོ་ན་བཞི་བཅུ་ཡིན་པའི་ཕ་ཉ           4.557  \n",
       "869   ི་མ་མོ་ཊ་གཏོང་རྒྱུར་དབྱིངས་འཕར་ཏེ་མགྱོགས་པོ་གཏ...           9.420  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ab_ga_id = 1\n",
    "group_ab_gb_id = 2\n",
    "group_ab_gc_id = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['group_id'] = group_ab_ga_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['state'] = 'imported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_length(st):\n",
    "    return len(st) < 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['inference_transcript'].apply(lambda x: len(x) < 500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum ID in the 'Task' table is: 420928\n"
     ]
    }
   ],
   "source": [
    "last_db_id = get_max_db_id()\n",
    "\n",
    "id_arr = list(range(last_db_id + 1, df.shape[0] + last_db_id + 1))\n",
    "\n",
    "df['id'] = id_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>url</th>\n",
       "      <th>inference_transcript</th>\n",
       "      <th>audio_duration</th>\n",
       "      <th>group_id</th>\n",
       "      <th>state</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7537</th>\n",
       "      <td>STT_AB00595_0001_21936_to_27346</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/stt_pech...</td>\n",
       "      <td>འགྲུལ་པ་དང་ལམ། ནང་མི་ཞིག་སྒེར་གྱི་མོ་ཊ་ཁྲིད་ནས...</td>\n",
       "      <td>5.410</td>\n",
       "      <td>1</td>\n",
       "      <td>imported</td>\n",
       "      <td>420929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>STT_AB00595_0002_28250_to_37056</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/stt_pech...</td>\n",
       "      <td>རྒྱབ་ཏུ་ཨ་མ་བཟང་མོ། ཕྲུ་གུ་འབྲིང་བ་བུ་དོན་གྲུ...</td>\n",
       "      <td>8.806</td>\n",
       "      <td>1</td>\n",
       "      <td>imported</td>\n",
       "      <td>420930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>STT_AB00595_0003_38523_to_41988</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/stt_pech...</td>\n",
       "      <td>ཕྲུ་གུ་ཚོའི་དབར་ལ་ལོ་གསུམ་རེའི་་ཁྱད་པར་ཡོད་པ་...</td>\n",
       "      <td>3.465</td>\n",
       "      <td>1</td>\n",
       "      <td>imported</td>\n",
       "      <td>420931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8450</th>\n",
       "      <td>STT_AB00595_0004_59854_to_64411</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/stt_pech...</td>\n",
       "      <td>་ལོ་ན་བཞི་བཅུ་ཡིན་པའི་ཕ་ཉ</td>\n",
       "      <td>4.557</td>\n",
       "      <td>1</td>\n",
       "      <td>imported</td>\n",
       "      <td>420932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>STT_AB00595_0005_77875_to_87295</td>\n",
       "      <td>https://d38pmlk0v88drf.cloudfront.net/stt_pech...</td>\n",
       "      <td>ི་མ་མོ་ཊ་གཏོང་རྒྱུར་དབྱིངས་འཕར་ཏེ་མགྱོགས་པོ་གཏ...</td>\n",
       "      <td>9.420</td>\n",
       "      <td>1</td>\n",
       "      <td>imported</td>\n",
       "      <td>420933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file_name  \\\n",
       "7537  STT_AB00595_0001_21936_to_27346   \n",
       "767   STT_AB00595_0002_28250_to_37056   \n",
       "3194  STT_AB00595_0003_38523_to_41988   \n",
       "8450  STT_AB00595_0004_59854_to_64411   \n",
       "869   STT_AB00595_0005_77875_to_87295   \n",
       "\n",
       "                                                    url  \\\n",
       "7537  https://d38pmlk0v88drf.cloudfront.net/stt_pech...   \n",
       "767   https://d38pmlk0v88drf.cloudfront.net/stt_pech...   \n",
       "3194  https://d38pmlk0v88drf.cloudfront.net/stt_pech...   \n",
       "8450  https://d38pmlk0v88drf.cloudfront.net/stt_pech...   \n",
       "869   https://d38pmlk0v88drf.cloudfront.net/stt_pech...   \n",
       "\n",
       "                                   inference_transcript  audio_duration  \\\n",
       "7537  འགྲུལ་པ་དང་ལམ། ནང་མི་ཞིག་སྒེར་གྱི་མོ་ཊ་ཁྲིད་ནས...           5.410   \n",
       "767    རྒྱབ་ཏུ་ཨ་མ་བཟང་མོ། ཕྲུ་གུ་འབྲིང་བ་བུ་དོན་གྲུ...           8.806   \n",
       "3194   ཕྲུ་གུ་ཚོའི་དབར་ལ་ལོ་གསུམ་རེའི་་ཁྱད་པར་ཡོད་པ་...           3.465   \n",
       "8450                          ་ལོ་ན་བཞི་བཅུ་ཡིན་པའི་ཕ་ཉ           4.557   \n",
       "869   ི་མ་མོ་ཊ་གཏོང་རྒྱུར་དབྱིངས་འཕར་ཏེ་མགྱོགས་པོ་གཏ...           9.420   \n",
       "\n",
       "      group_id     state      id  \n",
       "7537         1  imported  420929  \n",
       "767          1  imported  420930  \n",
       "3194         1  imported  420931  \n",
       "8450         1  imported  420932  \n",
       "869          1  imported  420933  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['file_name'] != 'STT_AB00609_0001_1134_to_7278']\n",
    "\n",
    "df = df[df['file_name'] != 'STT_AB00609_0002_8643_to_11203']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('stt_ab_upload.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "have = '/home/monlamai/_Task__202401231441.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_files = open(have).read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['file_name'].isin(have_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9354, 7)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_files = have_files[1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
